{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8aa8265-90c8-4866-996b-1bb70b414e43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load bronze tables from Delta Lake into Spark DataFrames\n",
    "bronze_schema = \"workspace.stock_project\"\n",
    "silver_schema = \"workspace.stock_project\"\n",
    "\n",
    "df_stock = spark.table(f\"{bronze_schema}.bronze_stock_prices\")           # Stock price history\n",
    "df_port = spark.table(f\"{bronze_schema}.bronze_portfolio_transactions\")  # Portfolio transactions\n",
    "df_company = spark.table(f\"{bronze_schema}.bronze_company_sector\")       # Company sector metadata\n",
    "df_bench = spark.table(f\"{bronze_schema}.bronze_benchmark_index\")        # Benchmark index data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59ceb2bf-2331-4f7d-8f43-4f15d7d0daee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**stock prices cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ae75fe9-d8e5-4505-b614-ebd5e4b8fa04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Standardize missing values and fix column data types for stock price data\n",
    "df_stock1 = (\n",
    "    df_stock\n",
    "    .replace(\"NaN\", None)  # Replace string \"NaN\" with nulls\n",
    "    .withColumn(\"open\", col(\"open\").cast(\"double\"))    # Cast 'open' to double\n",
    "    .withColumn(\"high\", col(\"high\").cast(\"double\"))    # Cast 'high' to double\n",
    "    .withColumn(\"low\", col(\"low\").cast(\"double\"))      # Cast 'low' to double\n",
    "    .withColumn(\"close\", col(\"close\").cast(\"double\"))  # Cast 'close' to double\n",
    "    .withColumn(\"volume\", col(\"volume\").cast(\"long\"))  # Cast 'volume' to long\n",
    "    .withColumn(\"date\", col(\"date\").cast(\"date\"))      # Cast 'date' to date\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55763c0f-4fc4-43c4-94be-89e02b97a61c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#validation rules\n",
    "df_stock2 = (\n",
    "    df_stock1\n",
    "    .withColumn(\"rule_null_price\",       col(\"open\").isNull() | col(\"close\").isNull())\n",
    "    .withColumn(\"rule_null_volume\",      col(\"volume\").isNull())\n",
    "    .withColumn(\"rule_null_high_low\",    col(\"high\").isNull() | col(\"low\").isNull())\n",
    "    .withColumn(\"rule_negative_volume\",  col(\"volume\") < 0)\n",
    "    .withColumn(\"rule_zero_close\",       col(\"close\") == 0)\n",
    "    .withColumn(\"rule_close_outside\",   (col(\"close\") < col(\"low\")) | (col(\"close\") > col(\"high\")))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68bbe182-622f-4208-be19-67c68820eab0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter rows violating validation rules for stock prices\n",
    "df_stock_invalid = df_stock2.filter(\n",
    "    \"rule_null_price OR rule_null_volume OR rule_negative_volume OR rule_zero_close OR rule_close_outside OR rule_null_high_low\"\n",
    ")\n",
    "\n",
    "# Save invalid stock price rows to Delta table\n",
    "df_stock_invalid.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{silver_schema}.silver_invalid_stock_prices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52297bf6-3c7b-4fde-b6cc-55f8aec412b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean stock price table by filtering out invalid rows, fixing swapped high/low values, and removing duplicates\n",
    "df_stock_clean = (\n",
    "    df_stock2\n",
    "    .filter(~(\n",
    "        col(\"rule_null_price\") | \n",
    "        col(\"rule_null_volume\") | \n",
    "        col(\"rule_negative_volume\") | \n",
    "        col(\"rule_zero_close\") | \n",
    "        col(\"rule_close_outside\") |\n",
    "        col(\"rule_null_high_low\")\n",
    "    ))\n",
    "    # Correct cases where 'high' is less than 'low'\n",
    "    .withColumn(\"fixed_high\", when(col(\"high\") < col(\"low\"), col(\"low\")).otherwise(col(\"high\")))\n",
    "    .withColumn(\"fixed_low\", when(col(\"high\") < col(\"low\"), col(\"high\")).otherwise(col(\"low\")))\n",
    "    .drop(\"high\", \"low\")\n",
    "    .withColumnRenamed(\"fixed_high\", \"high\")\n",
    "    .withColumnRenamed(\"fixed_low\", \"low\")\n",
    "    .dropDuplicates()\n",
    "    # Remove validation rule columns\n",
    "    .drop(\n",
    "        \"rule_null_price\",\n",
    "        \"rule_null_volume\",\n",
    "        \"rule_negative_volume\",\n",
    "        \"rule_zero_close\",\n",
    "        \"rule_null_high_low\",\n",
    "        \"rule_close_outside\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Save cleaned stock price data to Delta table, partitioned by ticker\n",
    "df_stock_clean.write.mode(\"overwrite\").partitionBy(\"Ticker\").format(\"delta\").saveAsTable(f\"{silver_schema}.silver_stock_prices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b78d103d-aff9-44bb-9cb3-3698b159a207",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**company sector cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e58b3d5-8273-4bf1-bbca-1c4856dc6451",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Correct sector misspellings and fill nulls with 'Other'\n",
    "df_company1 = (\n",
    "    df_company\n",
    "    .replace({\"Technlogy\": \"Technology\"})\n",
    "    .withColumn(\"sector\", when(col(\"sector\").isNull(), \"Other\").otherwise(col(\"sector\")))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0af3c09d-b6c2-44eb-856c-f77832d24d02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select distinct tickers from cleaned stock prices\n",
    "valid_tickers = df_stock_clean.select(\"ticker\").distinct()\n",
    "\n",
    "# Identify companies with tickers not present in the stock price dataset\n",
    "df_company_invalid = df_company1.join(valid_tickers, \"ticker\", \"left_anti\")\n",
    "\n",
    "# Identify companies with tickers present in the stock price dataset\n",
    "df_company_valid = df_company1.join(valid_tickers, \"ticker\", \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4636fbb-a1d9-41b9-bf35-e93d3a33db91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save companies with tickers not present in cleaned stock prices to Delta table\n",
    "df_company_invalid.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{silver_schema}.silver_invalid_company_sector\")\n",
    "\n",
    "# Save companies with valid tickers to Delta table\n",
    "df_company_valid.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{silver_schema}.silver_company_sector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e83b6a62-62f3-411f-ab81-f00698c9fe9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**portfolio transactions cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bc5c75a-c75b-4014-92b3-2ee232235633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Standardize portfolio transactions: replace 'NaN' with nulls, and cast columns to correct types\n",
    "df_port1 = (\n",
    "    df_port\n",
    "    .replace(\"NaN\", None)\n",
    "    .withColumn(\"price\", col(\"price\").cast(\"double\"))\n",
    "    .withColumn(\"quantity\", col(\"quantity\").cast(\"int\"))\n",
    "    .withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72fdeb7e-57b3-4706-a090-127cf1fdb83b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Correct misspelled 'BUYY' in the 'action' column to 'BUY'\n",
    "df_port2 = (\n",
    "    df_port1\n",
    "    .withColumn(\n",
    "        \"action\",\n",
    "        when(col(\"action\") == \"BUYY\", \"BUY\")\n",
    "        .otherwise(col(\"action\"))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "871ed875-5317-473b-af03-b6389efb1af2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Identify invalid portfolio transaction rows based on nulls, non-positive values, invalid actions, or future dates\n",
    "from pyspark.sql.functions import current_date\n",
    "\n",
    "invalid_condition = (\n",
    "    (col(\"price\").isNull()) |                # price is null\n",
    "    (col(\"quantity\").isNull()) |             # quantity is null\n",
    "    (col(\"quantity\") <= 0) |                 # quantity is non-positive\n",
    "    (col(\"price\") <= 0) |                    # price is non-positive\n",
    "    (~col(\"action\").isin(\"BUY\", \"SELL\")) |   # action is not BUY or SELL\n",
    "    (col(\"date\") > current_date())           # date is in the future\n",
    ")\n",
    "\n",
    "df_port_invalid = df_port2.filter(invalid_condition)\n",
    "\n",
    "# Save invalid portfolio transactions to Delta table\n",
    "df_port_invalid.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{silver_schema}.silver_invalid_portfolio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea267d9d-43aa-4416-a00b-35a470cbdf02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter out invalid portfolio transactions to obtain valid rows\n",
    "df_port_clean = df_port2.filter(~invalid_condition)\n",
    "\n",
    "# Save valid portfolio transactions to Delta table\n",
    "df_port_clean.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{silver_schema}.silver_portfolio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecfe53cc-42d5-49d9-86f3-13019b25432b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**benchmark index cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fc4da2f-dcd3-44dc-9525-c652b69d4b8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Standardize benchmark index: replace 'NaN' with nulls, cast 'date' to date type, and 'close' to double\n",
    "df_bench1 = (\n",
    "    df_bench\n",
    "    .replace(\"NaN\", None)\n",
    "    .withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
    "    .withColumn(\"close\", col(\"close\").cast(\"double\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "251224d9-6670-4b38-86d5-da4947264914",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Add a constant key column for window partitioning\n",
    "df_bench1_keyed = df_bench1.withColumn(\"key\", F.lit(1))\n",
    "\n",
    "# Define window for calculating previous close\n",
    "w = Window.partitionBy(\"key\").orderBy(\"date\")\n",
    "\n",
    "# Calculate previous close, daily return, and flag extreme/invalid values\n",
    "df_bench2 = (\n",
    "    df_bench1_keyed\n",
    "    .withColumn(\"prev_close\", F.lag(\"close\").over(w))  # Previous day's close\n",
    "    .withColumn(\n",
    "        \"daily_return\",\n",
    "        (col(\"close\") - col(\"prev_close\")) / col(\"prev_close\")  # Daily return\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"extreme_return_flag\",\n",
    "        when(col(\"daily_return\") < -0.20, True).otherwise(False)  # Flag extreme negative returns\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"invalid_open_flag\",\n",
    "        when(col(\"open\").isNull() | (col(\"open\") <= 0), True).otherwise(False)  # Flag invalid open prices\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"invalid_close_flag\",\n",
    "        when(col(\"close\").isNull() | (col(\"close\") <= 0), True).otherwise(False)  # Flag invalid close prices\n",
    "    )\n",
    ")\n",
    "\n",
    "# Filter rows with invalid open/close or missing daily return\n",
    "df_bench_invalid = df_bench2.filter(\n",
    "    (col(\"invalid_open_flag\") == True) |\n",
    "    (col(\"invalid_close_flag\") == True) |\n",
    "    (col(\"daily_return\").isNull())\n",
    ")\n",
    "\n",
    "# Save invalid benchmark index rows to Delta table\n",
    "df_bench_invalid.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .saveAsTable(f\"{silver_schema}.silver_invalid_benchmark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75998ad0-38b2-404e-be21-98a3a37e7498",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter benchmark index for valid rows (open/close > 0 and not null), drop helper columns, and save to Delta table\n",
    "df_bench_clean = df_bench2.filter(\n",
    "    (col(\"invalid_open_flag\") == False) &\n",
    "    (col(\"invalid_close_flag\") == False) \n",
    ").drop(\"prev_close\", \"key\")\n",
    "\n",
    "df_bench_clean.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\n",
    "    f\"{silver_schema}.silver_benchmark_index\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a60039e-0c5e-437a-91ca-57eead9bbfa5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Silver data Quality Summary Log**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5b548eb-0c24-47f6-9515-07a11df77f79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>table</th><th>row_count</th><th>unique_rows</th><th>null_count</th></tr></thead><tbody><tr><td>silver_stock_prices</td><td>62640</td><td>62640</td><td>0</td></tr><tr><td>silver_portfolio</td><td>29796</td><td>29796</td><td>0</td></tr><tr><td>silver_company_sector</td><td>30</td><td>30</td><td>0</td></tr><tr><td>silver_benchmark_index</td><td>2025</td><td>2025</td><td>22</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "silver_stock_prices",
         62640,
         62640,
         0
        ],
        [
         "silver_portfolio",
         29796,
         29796,
         0
        ],
        [
         "silver_company_sector",
         30,
         30,
         0
        ],
        [
         "silver_benchmark_index",
         2025,
         2025,
         22
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "table",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "row_count",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "unique_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "null_count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data quality summary function: returns a DataFrame with table name, row count, unique row count, and total null count\n",
    "def dq(df, name):\n",
    "    return spark.createDataFrame([\n",
    "        (\n",
    "            name,  # Table name\n",
    "            df.count(),  # Total row count\n",
    "            df.dropDuplicates().count(),  # Unique row count\n",
    "            sum(df.filter(col(c).isNull()).count() for c in df.columns)  # Total null values across all columns\n",
    "        )\n",
    "    ], [\"table\", \"row_count\", \"unique_rows\", \"null_count\"])\n",
    "\n",
    "# Aggregate data quality logs for all silver tables\n",
    "dq_log = (\n",
    "    dq(df_stock_clean, \"silver_stock_prices\")\n",
    "    .union(dq(df_port_clean, \"silver_portfolio\"))\n",
    "    .union(dq(df_company_valid, \"silver_company_sector\"))\n",
    "    .union(dq(df_bench_clean, \"silver_benchmark_index\"))\n",
    ")\n",
    "\n",
    "display(dq_log)  # Display data quality summary\n",
    "\n",
    "# Save data quality log to Delta table\n",
    "dq_log.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{silver_schema}.silver_dq_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de208986-19ad-4603-998e-88637b946e1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exit the notebook with a status message for workflow orchestration\n",
    "dbutils.notebook.exit(\"SUCCESS\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_cleaning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}